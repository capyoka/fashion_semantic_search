import logging
import time
import asyncio
from fastapi import APIRouter, Response
from app.schemas.request import SearchRequest
from app.schemas.response import SearchResponse
from app.core.config import settings
from app.services.aembedder import AsyncEmbedder
from app.services.agenerator import AsyncGenerator
from app.services.retriever import HybridRetriever
from app.services.bm25_sparse import BM25Sparse
from qdrant_client import QdrantClient

# Loggerè¨­å®š
logger = logging.getLogger(__name__)

router = APIRouter(tags=["rag"])

# ============================================================
# åˆæœŸåŒ–ï¼šã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ»Retriever æº–å‚™
# ============================================================

# éåŒæœŸ LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
_gen = AsyncGenerator(settings.OPENAI_CHAT_MODEL, settings.OPENAI_CHAT_FAST_MODEL)
_embed = AsyncEmbedder(settings.OPENAI_EMBED_MODEL)

# Qdrantï¼ˆãƒ­ãƒ¼ã‚«ãƒ« or Remoteï¼‰
_qdr = QdrantClient(path=settings.QDRANT_PATH)

# BM25 ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
_bm25 = BM25Sparse.load_from_qdrant(_qdr, settings.QDRANT_COLLECTION)

# Retriever æº–å‚™
_ret = HybridRetriever(_qdr, settings.QDRANT_COLLECTION, bm25_indexer=_bm25)

# ============================================================
# æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================
@router.post("/search", response_model=SearchResponse)
async def search(req: SearchRequest, resp: Response):
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ (dense + sparse RRFèåˆ) + LLMãƒªãƒ©ãƒ³ã‚¯
    """
    start_time = time.time()
    user_q = (req.query or "").strip()
    top_k = req.top_k or 30
    
    logger.info(f"ğŸ” Search request started - Query: '{user_q}', Top-K: {top_k}")
    logger.debug(f"Request details - User-Agent: {req.__dict__}")

    # 1ï¸.ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆï¼ˆrewrite ã®ã¿å®Ÿè¡Œï¼‰
    query_start = time.time()
    logger.info("ğŸ“ Step 1: Starting query rewrite...")
    try:
        rewritten, keywords = await _gen.rewrite_query(user_q)
        rewritten = rewritten.strip() if rewritten else user_q
        keywords = keywords.strip() if keywords else user_q
        query_time = time.time() - query_start
        logger.info(f"âœ… Query rewrite completed in {query_time:.3f}s")
        logger.info(f"   Original: '{user_q}'")
        logger.info(f"   Semantic: '{rewritten}'")
        logger.info(f"   Keywords: '{keywords}'")
    except Exception as e:
        logger.error(f"âŒ Query rewrite failed: {e}")
        rewritten, keywords = user_q, user_q

    # 2ï¸.åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆéåŒæœŸï¼‰
    embed_start = time.time()
    logger.info("ğŸ§  Step 2: Generating embeddings...")
    try:
        qvec = (await _embed.encode([rewritten]))[0]
        embed_time = time.time() - embed_start
        logger.info(f"âœ… Embedding generated in {embed_time:.3f}s (dim: {len(qvec)})")
    except Exception as e:
        logger.error(f"âŒ Embedding generation failed: {e}")
        raise

    # 3ï¸.ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆåŒæœŸQdrant â†’ ã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œï¼‰
    search_start = time.time()
    logger.info("ğŸ” Step 3: Performing hybrid search...")
    try:
        docs = await asyncio.to_thread(_ret.search, keywords, qvec, top_k)
        search_time = time.time() - search_start
        logger.info(f"âœ… Hybrid search completed in {search_time:.3f}s")
    except Exception as e:
        logger.error(f"âŒ Hybrid search failed: {e}")
        raise

    # BM25ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿Pointï¼ˆ__bm25_meta__ï¼‰ã‚’é™¤å¤–
    original_count = len(docs)
    docs = [d for d in docs if not d.get("_bm25_meta")]
    filtered_count = len(docs)
    if original_count != filtered_count:
        logger.debug(f"Filtered out {original_count - filtered_count} metadata points")

    logger.info(f"ğŸ“Š Retrieved {len(docs)} documents from search")

    if not docs:
        total_time = time.time() - start_time
        logger.warning(f"âš ï¸ No documents found for query. Total time: {total_time:.3f}s")
        return {"results": []}

    # 4ï¸.LLM ãƒªãƒ©ãƒ³ã‚¯ï¼ˆéåŒæœŸãƒ»å®‰å…¨åŒ–ï¼‰
    rerank_start = time.time()
    logger.info("ğŸ”„ Step 4: Starting LLM reranking...")
    try:
        order = await _gen.rerank(user_q, docs)
        valid_order = [i for i in order if 0 <= i < len(docs)]
        order = valid_order or list(range(len(docs)))
        docs_r = [docs[i] for i in order]
        rerank_time = time.time() - rerank_start
        logger.info(f"âœ… Reranking completed in {rerank_time:.3f}s")
        logger.debug(f"Rerank order: {order[:10]}{'...' if len(order) > 10 else ''}")
    except Exception as e:
        rerank_time = time.time() - rerank_start
        logger.warning(f"âš ï¸ Rerank failed after {rerank_time:.3f}s: {e}")
        logger.info("Using original search order as fallback")
        docs_r = docs

    # 5.ä¸Šä½ top_k çµæœã‚’è¿”ã™
    final_results = docs_r[:top_k]
    total_time = time.time() - start_time
    
    logger.info(f"ğŸ¯ Step 5: Returning {len(final_results)} final results")
    logger.info(f"â±ï¸ Total search time: {total_time:.3f}s")
    
    # çµæœã®è©³ç´°ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ï¼‰
    if logger.isEnabledFor(logging.DEBUG):
        for i, doc in enumerate(final_results[:3]):  # ä¸Šä½3ä»¶ã®ã¿
            logger.debug(f"Result {i+1}: ID={doc.get('id', 'N/A')}, Score={doc.get('score', 'N/A'):.4f}")
            logger.debug(f"  Title: {doc.get('title', 'N/A')[:100]}...")
    
    return {"results": final_results}
